0. Create a compute instance for the Workspace in AML (not actually necessary?)
    https://learn.microsoft.com/en-us/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&tabs=azure-studio


1. Register the Model in AML
    az ml model create \
    --name my-fine-tuned-bert \
    --version 1 \
    --path /Users/alinstefanescu/Documents/code-stats/services/inference/fine_tuned_bert \
    --resource-group code_stats_paying \
    --workspace-name code_stats_paying_workspace

2. Create a Scoring Script
    To deploy your model, you need a scoring script that defines how the model processes input and generates predictions.


3. Create an Environment
    The environment specifies the dependencies required for your model and scoring script. 
    You can either use a predefined Azure environment or create a custom one.


    - Create YAML file as in environment.yaml 

    - Create Dockerfile
    - build & push docker image to Docker Hub
        docker build -t my-azureml-inference-image .
        docker push my-azureml-inference-image
    - Create environment from Docker image & yml file:

    az ml environment create \
    --name my-inference-env \
    --conda-file /Users/alinstefanescu/Documents/code-stats/services/inference/environment.yml \
    --resource-group code_stats_paying \
    --workspace-name code_stats_paying_workspace

    az ml environment create \
    --name my-inference-env \
    --version 1 \
    --file  /Users/alinstefanescu/Documents/code-stats/services/inference/environment.yml  \
    --image pytorch/pytorch \
    --resource-group code_stats_paying \
    --workspace-name code_stats_paying_workspace

    (+ YAML)

    When you create the environment, Azure will:
        Parse the environment.yml file.
        Build a Docker image for your compute target with all specified dependencies installed.
        Store the environment as a reusable artifact in your Azure ML workspace.

4. Create a LOCAL deployment of the model for testing (only possible through CLI / )
    NEED  TO HAVE DOCKER & CONDA!!!!!!
    See examples here: https://github.com/Azure/azureml-examples/tree/main/cli/endpoints/online/deploy-with-packages/custom-model
     - Do a YML file for the local endpoint in v2 schema:
         
    - Create the local endpoint
        ENDPOINT_NAME=my-local-bert-endpoint

            az ml online-endpoint create \
            --local \
            -n local-endpoint \
            -f /Users/alinstefanescu/Documents/code-stats/services/azure-example/endpoint.yml \
            --resource-group code_stats_paying \
            --workspace-name code_stats_paying_workspace

        Verify:
            az ml online-endpoint list \
            --local \
            --resource-group code_stats_paying \
            --workspace-name code_stats_paying_workspace

        By default, it usually listens on http://localhost:5001/score (unless you specify a different port). You can send HTTP requests to this local endpoint, and it will handle inference using the locally stored model.
        For example, when you make a request with curl:
            curl -X POST http://localhost:5001/score -H "Content-Type: application/json" -d '{"text": "This is a test sentence."}'

    - Create a yml v2 file for the deployment, which should reference the initial environment.yaml
        It will also use a default ubuntu image for the environment, on top of which the dependencies are installed.
        (see local_deployment.yml)
        
    - Create local deployment of the local endpoint:
        -> See local_deployment.yml

            az ml online-deployment create \
            --local \
            -n example-deployment \
            -f /Users/alinstefanescu/Documents/code-stats/services/azure-example/blue-deployment.yml \
            --resource-group code_stats_paying \
            --workspace-name code_stats_paying_workspace

        Verify: 
            az ml online-deployment show \
            --local \
            -n example-deployment \
            --resource-group code_stats_paying \
            --workspace-name code_stats_paying_workspace \
            --endpoint-name example-endpoint

        Invoke:
           az ml online-endpoint invoke \
           --local \
           --name example-endpoint \
           --request-file /Users/alinstefanescu/Documents/code-stats/services/azure-example/sample-request.json \
           --resource-group code_stats_paying \
           --workspace-name code_stats_paying_workspace

        !!!!!If you want to use a REST client (like curl), you must have the scoring URI. 
        To get the scoring URI, run:
            az ml online-endpoint show --local -n example-endpoint
            Ex: http://localhost:55024/score
         In the returned data, find the scoring_uri attribute.

        In the example score.py file, the run() method logs some output to the console.
        You can view this output by using the get-logs command:
            az ml online-deployment get-logs --local -n example-deployment --endpoint example-endpoint

4. Deploy the Model as an Online Endpoint
    az ml online-endpoint create \
    --name my-bert-endpoint \
    --resource-group code_stats_paying \
    --workspace-name code_stats_paying_workspace 
    --type Kubernetes

    az ml online-deployment create \
    --name my-deployment \
    --endpoint-name my-endpoint \
    --model my-trained-model:1 \
    --environment my-env:1 \
    --entry-script score.py \
    --instance-type defaultinstancetype \
    --resource-group code_stats_paying \
    --workspace-name code_stats_paying_workspace

5. Test the Endpoint
    az ml online-endpoint invoke \
    --name my-endpoint \
    --request-file request.json \
    --resource-group code_stats_paying \
    --workspace-name code_stats_paying_workspace

6. Configure Scaling (Optional)
    az ml online-deployment update \
    --name my-deployment \
    --endpoint-name my-endpoint \
    --scale-settings-scale-type Automatic \
    --scale-settings-min-instances 1 \
    --scale-settings-max-instances 5

7. Monitor the Endpoint
